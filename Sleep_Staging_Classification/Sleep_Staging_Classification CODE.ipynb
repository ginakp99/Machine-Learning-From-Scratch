{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7a8eYzvS3-F",
        "outputId": "f2b8b5db-77e1-41a2-e121-cb046bf887a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Loading Data ---\n",
            "Success! Train shape: (33723, 16), Test shape: (13900, 16)\n",
            "\n",
            "--- Task 1.1: Class Frequencies ---\n",
            "Class      | Count      | Frequency (%)  \n",
            "---------------------------------------------\n",
            "0.0        | 17565      | 52.09          \n",
            "1.0        | 3221       | 9.55           \n",
            "2.0        | 8523       | 25.27          \n",
            "3.0        | 1583       | 4.69           \n",
            "4.0        | 2831       | 8.39           \n",
            "\n",
            "--- Task 1.2.1: Logistic Regression ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Error: 0.1495\n",
            "Test Error:     0.0995\n",
            "\n",
            "--- Task 1.2.2: Random Forests ---\n",
            "RF (50 trees) -> Train Error: 0.0001 | Test Error: 0.1122\n",
            "RF (100 trees) -> Train Error: 0.0000 | Test Error: 0.1111\n",
            "RF (200 trees) -> Train Error: 0.0000 | Test Error: 0.1109\n",
            "\n",
            "--- Task 1.2.3: KNN with Cross-Validation ---\n",
            "Running Cross-Validation (this might take a minute)...\n",
            "Optimal k found: 20\n",
            "KNN (k=20)   -> Train Error: 0.1405 | Test Error: 0.1024\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ==========================================\n",
        "# PART 0: LOAD DATA\n",
        "# ==========================================\n",
        "print(\"--- Loading Data ---\")\n",
        "try:\n",
        "    # Load CSV files into NumPy arrays\n",
        "    # We use .values to convert from Pandas DataFrame to NumPy array\n",
        "    # .ravel() flattens the target arrays (y) to a 1D list, which sklearn prefers\n",
        "    X_train = pd.read_csv(\"X_train.csv\").values\n",
        "    y_train = pd.read_csv(\"y_train.csv\").values.ravel()\n",
        "    X_test = pd.read_csv(\"X_test.csv\").values\n",
        "    y_test = pd.read_csv(\"y_test.csv\").values.ravel()\n",
        "    print(f\"Success! Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: Files not found. Please make sure you uploaded:\")\n",
        "    print(\"X_train.csv, y_train.csv, X_test.csv, y_test.csv\")\n",
        "    # Stop execution if data is missing\n",
        "    raise\n",
        "\n",
        "# ==========================================\n",
        "# TASK 1.1: CLASS FREQUENCIES\n",
        "# ==========================================\n",
        "print(\"\\n--- Task 1.1: Class Frequencies ---\")\n",
        "\n",
        "# Count how many times each class appears (0, 1, 2, 3, 4)\n",
        "unique_classes, counts = np.unique(y_train, return_counts=True)\n",
        "total_samples = len(y_train)\n",
        "\n",
        "print(f\"{'Class':<10} | {'Count':<10} | {'Frequency (%)':<15}\")\n",
        "print(\"-\" * 45)\n",
        "\n",
        "for cls, count in zip(unique_classes, counts):\n",
        "    frequency = (count / total_samples) * 100\n",
        "    print(f\"{cls:<10} | {count:<10} | {frequency:<15.2f}\")\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# PREPROCESSING (SCALING)\n",
        "# ==========================================\n",
        "# Logistic Regression and KNN need data to be \"scaled\" (normalized).\n",
        "# We fit the scaler on Training data and transform both Train and Test.\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Helper function to calculate \"0-1 Loss\" (Error Rate)\n",
        "def get_error_rate(model, X, y):\n",
        "    y_pred = model.predict(X)\n",
        "    return 1.0 - accuracy_score(y, y_pred)\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# TASK 1.2.1: LOGISTIC REGRESSION\n",
        "# ==========================================\n",
        "print(\"\\n--- Task 1.2.1: Logistic Regression ---\")\n",
        "\n",
        "# Initialize model\n",
        "# 'multinomial' = Multi-class classification\n",
        "# 'C=1.0' = Default L2 regularization\n",
        "log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=2000, C=1.0, random_state=42)\n",
        "\n",
        "# Train (Fit) the model\n",
        "log_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Calculate Errors\n",
        "train_err = get_error_rate(log_reg, X_train_scaled, y_train)\n",
        "test_err = get_error_rate(log_reg, X_test_scaled, y_test)\n",
        "\n",
        "print(f\"Training Error: {train_err:.4f}\")\n",
        "print(f\"Test Error:     {test_err:.4f}\")\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# TASK 1.2.2: RANDOM FORESTS\n",
        "# ==========================================\n",
        "print(\"\\n--- Task 1.2.2: Random Forests ---\")\n",
        "\n",
        "# List of tree counts to test\n",
        "n_trees_list = [50, 100, 200]\n",
        "\n",
        "for n in n_trees_list:\n",
        "    # Initialize Random Forest\n",
        "    # We use X_train (unscaled) because Trees don't strictly need scaling\n",
        "    rf = RandomForestClassifier(n_estimators=n, random_state=42, n_jobs=-1)\n",
        "    rf.fit(X_train, y_train)\n",
        "\n",
        "    # Calculate Errors\n",
        "    train_err = get_error_rate(rf, X_train, y_train)\n",
        "    test_err = get_error_rate(rf, X_test, y_test)\n",
        "\n",
        "    print(f\"RF ({n} trees) -> Train Error: {train_err:.4f} | Test Error: {test_err:.4f}\")\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# TASK 1.2.3: K-NEAREST NEIGHBORS (CV)\n",
        "# ==========================================\n",
        "print(\"\\n--- Task 1.2.3: KNN with Cross-Validation ---\")\n",
        "\n",
        "# We want to test k values from 1 to 20\n",
        "k_candidates = range(1, 21)\n",
        "cv_accuracy_scores = []\n",
        "\n",
        "print(\"Running Cross-Validation (this might take a minute)...\")\n",
        "\n",
        "for k in k_candidates:\n",
        "    # Initialize KNN\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "\n",
        "    # Run 5-fold Cross-Validation\n",
        "    # This splits the training data 5 times and checks accuracy\n",
        "    scores = cross_val_score(knn, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "    # Store the average accuracy for this k\n",
        "    cv_accuracy_scores.append(scores.mean())\n",
        "\n",
        "# Find the K that had the highest accuracy\n",
        "best_index = np.argmax(cv_accuracy_scores)\n",
        "best_k = k_candidates[best_index]\n",
        "\n",
        "print(f\"Optimal k found: {best_k}\")\n",
        "\n",
        "# Train the FINAL model using the best k\n",
        "best_knn = KNeighborsClassifier(n_neighbors=best_k)\n",
        "best_knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Calculate Errors\n",
        "train_err = get_error_rate(best_knn, X_train_scaled, y_train)\n",
        "test_err = get_error_rate(best_knn, X_test_scaled, y_test)\n",
        "\n",
        "print(f\"KNN (k={best_k})   -> Train Error: {train_err:.4f} | Test Error: {test_err:.4f}\")"
      ]
    }
  ]
}